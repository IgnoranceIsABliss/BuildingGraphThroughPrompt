{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\–î–ù–°\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\–î–ù–°\\AppData\\Roaming\\Python\\Python310\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from transformers import AutoConfig\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "import dateparser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "import calendar\n",
    "from datetime import date\n",
    "import torch\n",
    "from fuzzywuzzy import process\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "def create_dataset(row):\n",
    "    if pd.notnull(row[\"client\"]):\n",
    "        return {\"text\": row[\"client\"], \"label\": 0}\n",
    "    elif pd.notnull(row[\"service\"]):\n",
    "        return {\"text\": row[\"service\"], \"label\": 1}\n",
    "    elif pd.notnull(row[\"management\"]):\n",
    "        return {\"text\": row[\"management\"], \"label\": 2}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "dataset = data.apply(create_dataset, axis=1).dropna()\n",
    "texts = [item[\"text\"] for item in dataset if item is not None]\n",
    "labels = [item[\"label\"] for item in dataset if item is not None]\n",
    "\n",
    "df = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
    "df.to_csv(\"prepared_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 498/498 [00:00<00:00, 1161.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.read_csv(\"prepared_dataset.csv\"))\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\–î–ù–°\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\–î–ù–°\\AppData\\Local\\Temp\\ipykernel_22620\\817690833.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 10/250 [00:53<18:22,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8678, 'grad_norm': 21.24707794189453, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 20/250 [01:39<18:08,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.475, 'grad_norm': 29.340612411499023, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 30/250 [02:25<17:15,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4357, 'grad_norm': 23.560733795166016, 'learning_rate': 1.76e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 40/250 [03:17<18:17,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2451, 'grad_norm': 27.26820945739746, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 50/250 [03:59<12:37,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5288, 'grad_norm': 55.626399993896484, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|‚ñà‚ñà        | 50/250 [04:12<12:37,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.40251654386520386, 'eval_runtime': 13.43, 'eval_samples_per_second': 7.446, 'eval_steps_per_second': 0.968, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 60/250 [05:16<16:37,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2648, 'grad_norm': 1.9486981630325317, 'learning_rate': 1.5200000000000002e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 70/250 [06:08<14:34,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4436, 'grad_norm': 38.96726608276367, 'learning_rate': 1.4400000000000001e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 80/250 [07:01<14:49,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1757, 'grad_norm': 6.650800704956055, 'learning_rate': 1.3600000000000002e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 90/250 [07:49<12:35,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3089, 'grad_norm': 1.3280328512191772, 'learning_rate': 1.2800000000000001e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 100/250 [08:31<10:17,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2723, 'grad_norm': 17.13654327392578, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 100/250 [08:44<10:17,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3814987540245056, 'eval_runtime': 13.009, 'eval_samples_per_second': 7.687, 'eval_steps_per_second': 0.999, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 110/250 [09:58<11:34,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3229, 'grad_norm': 4.082408428192139, 'learning_rate': 1.1200000000000001e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 120/250 [10:39<08:50,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1011, 'grad_norm': 8.343453407287598, 'learning_rate': 1.04e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 130/250 [11:19<08:00,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1318, 'grad_norm': 0.7078419327735901, 'learning_rate': 9.600000000000001e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 140/250 [11:59<07:23,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2594, 'grad_norm': 0.8041132092475891, 'learning_rate': 8.8e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 150/250 [12:38<06:11,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1357, 'grad_norm': 1.867431879043579, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 150/250 [12:51<06:11,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3822961449623108, 'eval_runtime': 13.114, 'eval_samples_per_second': 7.625, 'eval_steps_per_second': 0.991, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 160/250 [13:43<06:23,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1143, 'grad_norm': 0.08781363815069199, 'learning_rate': 7.2000000000000005e-06, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 170/250 [14:25<06:08,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1992, 'grad_norm': 115.04917907714844, 'learning_rate': 6.4000000000000006e-06, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 180/250 [15:17<05:59,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1249, 'grad_norm': 0.17550180852413177, 'learning_rate': 5.600000000000001e-06, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 190/250 [16:06<04:54,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0797, 'grad_norm': 0.2835739850997925, 'learning_rate': 4.800000000000001e-06, 'epoch': 3.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 200/250 [16:56<03:54,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3599, 'grad_norm': 35.17710876464844, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 200/250 [17:11<03:54,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4368026852607727, 'eval_runtime': 14.8559, 'eval_samples_per_second': 6.731, 'eval_steps_per_second': 0.875, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 210/250 [18:16<03:36,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1589, 'grad_norm': 84.49368286132812, 'learning_rate': 3.2000000000000003e-06, 'epoch': 4.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 220/250 [19:10<02:51,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0871, 'grad_norm': 0.26972344517707825, 'learning_rate': 2.4000000000000003e-06, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 230/250 [20:05<01:44,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1591, 'grad_norm': 0.15981104969978333, 'learning_rate': 1.6000000000000001e-06, 'epoch': 4.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 240/250 [20:59<00:52,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0799, 'grad_norm': 0.19177226722240448, 'learning_rate': 8.000000000000001e-07, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [21:51<00:00,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0549, 'grad_norm': 1.0548782348632812, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [22:28<00:00,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35640957951545715, 'eval_runtime': 15.425, 'eval_samples_per_second': 6.483, 'eval_steps_per_second': 0.843, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [22:53<00:00,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1373.4728, 'train_samples_per_second': 1.449, 'train_steps_per_second': 0.182, 'train_loss': 0.25546302723884584, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [22:53<00:00,  5.50s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.25546302723884584, metrics={'train_runtime': 1373.4728, 'train_samples_per_second': 1.449, 'train_steps_per_second': 0.182, 'total_flos': 129996307169280.0, 'train_loss': 0.25546302723884584, 'epoch': 5.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_rugpt3small\\\\tokenizer_config.json',\n",
       " './finetuned_rugpt3small\\\\special_tokens_map.json',\n",
       " './finetuned_rugpt3small\\\\vocab.json',\n",
       " './finetuned_rugpt3small\\\\merges.txt',\n",
       " './finetuned_rugpt3small\\\\added_tokens.json',\n",
       " './finetuned_rugpt3small\\\\tokenizer.json')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./finetuned_rugpt3small\")\n",
    "tokenizer.save_pretrained(\"./finetuned_rugpt3small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9384876489639282}]\n",
      "[{'label': 'LABEL_1', 'score': 0.7501918077468872}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"./finetuned_rugpt3small\", tokenizer=\"./finetuned_rugpt3small\")\n",
    "\n",
    "result = classifier(\"–ê–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ –£—Ä–∞–ª—å—Å–∫–∞—è —Ñ–æ–ª—å–≥–∞\")\n",
    "print(result)\n",
    "result = classifier(\"1–°: –î–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_month(input_month):\n",
    "    \"\"\"\n",
    "    –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –º–µ—Å—è—Ü–∞.\n",
    "    \"\"\"\n",
    "    month_mapping = {\n",
    "        \"—è–Ω–≤–∞—Ä—å\": \"01\", \"—è–Ω–≤–∞—Ä—è\": \"01\",\n",
    "        \"—Ñ–µ–≤—Ä–∞–ª—å\": \"02\", \"—Ñ–µ–≤—Ä–∞–ª—è\": \"02\",\n",
    "        \"–º–∞—Ä—Ç\": \"03\", \"–º–∞—Ä—Ç–∞\": \"03\",\n",
    "        \"–∞–ø—Ä–µ–ª—å\": \"04\", \"–∞–ø—Ä–µ–ª—è\": \"04\",\n",
    "        \"–º–∞–π\": \"05\", \"–º–∞—è\": \"05\",\n",
    "        \"–∏—é–Ω—å\": \"06\", \"–∏—é–Ω—è\": \"06\",\n",
    "        \"–∏—é–ª—å\": \"07\", \"–∏—é–ª—è\": \"07\",\n",
    "        \"–∞–≤–≥—É—Å—Ç\": \"08\", \"–∞–≤–≥—É—Å—Ç–∞\": \"08\",\n",
    "        \"—Å–µ–Ω—Ç—è–±—Ä—å\": \"09\", \"—Å–µ–Ω—Ç—è–±—Ä—è\": \"09\",\n",
    "        \"–æ–∫—Ç—è–±—Ä—å\": \"10\", \"–æ–∫—Ç—è–±—Ä—è\": \"10\",\n",
    "        \"–Ω–æ—è–±—Ä—å\": \"11\", \"–Ω–æ—è–±—Ä—è\": \"11\",\n",
    "        \"–¥–µ–∫–∞–±—Ä—å\": \"12\", \"–¥–µ–∫–∞–±—Ä—è\": \"12\"\n",
    "    }\n",
    "\n",
    "    corrected_month = process.extractOne(input_month.lower(), month_mapping.keys())\n",
    "    return month_mapping.get(corrected_month[0]) if corrected_month else None\n",
    "\n",
    "\n",
    "def extract_date(text):\n",
    "    doc = nlp(text)\n",
    "    dates = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
    "    if dates:\n",
    "        for date_str in dates:\n",
    "            date_obj = dateparser.parse(date_str)\n",
    "            if date_obj:\n",
    "                return date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    regex_patterns = {\n",
    "        \"interval\": r'\\b—Å\\s+([–∞-—è–ê-–Ø]+)\\s+(\\d{2})\\s+–ø–æ\\s+([–∞-—è–ê-–Ø]+)\\s+(\\d{2})\\b',\n",
    "        \"single_date\": [\n",
    "            r'\\b(\\d{1,2})\\.(\\d{1,2})\\.(\\d{2,4})\\b',\n",
    "            r'\\b(\\d{1,2})\\s+([–∞-—è–ê-–Ø]+)\\s+(\\d{2,4})\\b',\n",
    "            r'\\b(\\d{1,2})\\s+([–∞-—è–ê-–Ø]+)\\s+(\\d{2})\\s+–≥–æ–¥–∞\\b',\n",
    "            r'\\b(\\d{1,2})\\s+([–∞-—è–ê-–Ø]+)\\b'\n",
    "        ],\n",
    "        \"month_year\": r'\\b(?<!\\d\\s)([–∞-—è–ê-–Ø]+)\\s+(\\d{2,4})\\b'\n",
    "    }\n",
    "\n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –¥–∞—Ç\n",
    "    interval_match = re.search(regex_patterns[\"interval\"], text, re.IGNORECASE)\n",
    "    if interval_match:\n",
    "        start_month_name, start_year, end_month_name, end_year = interval_match.groups()\n",
    "        start_month = correct_month(start_month_name)\n",
    "        end_month = correct_month(end_month_name)\n",
    "        if start_month and end_month:\n",
    "            if len(start_year) == 2:\n",
    "                start_year = f\"20{start_year}\"\n",
    "            if len(end_year) == 2:\n",
    "                end_year = f\"20{end_year}\"\n",
    "            start_date_obj = date(int(start_year), int(start_month), 1)\n",
    "            last_day = calendar.monthrange(int(end_year), int(end_month))[1]\n",
    "            end_date_obj = date(int(end_year), int(end_month), last_day)\n",
    "            return f\"{start_date_obj.strftime('%Y-%m-%d')} - {end_date_obj.strftime('%Y-%m-%d')}\"\n",
    "\n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –¥–∞—Ç\n",
    "    for pattern in regex_patterns[\"single_date\"]:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            if len(match.groups()) == 3:\n",
    "                day, month_or_name, year = match.groups()\n",
    "                month = correct_month(month_or_name)\n",
    "                if month:\n",
    "                    if len(year) == 2:\n",
    "                        year = f\"20{year}\"\n",
    "                    return f\"{year}-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "            elif len(match.groups()) == 2:  # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–Ω—è –∏ –º–µ—Å—è—Ü–∞ –±–µ–∑ –≥–æ–¥–∞\n",
    "                day, month_or_name = match.groups()\n",
    "                month = correct_month(month_or_name)\n",
    "                if month:\n",
    "                    year = datetime.now().year  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π –≥–æ–¥\n",
    "                    return f\"{year}-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "\n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–µ—Å—è—Ü–∞ –∏ –≥–æ–¥–∞\n",
    "    month_year_match = re.search(regex_patterns[\"month_year\"], text, re.IGNORECASE)\n",
    "    if month_year_match:\n",
    "        month_name, year = month_year_match.groups()\n",
    "        month = correct_month(month_name)\n",
    "        if month:\n",
    "            if len(year) == 2:\n",
    "                year = f\"20{year}\"\n",
    "            start_date_obj = date(int(year), int(month), 1)\n",
    "            last_day = calendar.monthrange(int(year), int(month))[1]\n",
    "            end_date_obj = date(int(year), int(month), last_day)\n",
    "            return f\"{start_date_obj.strftime('%Y-%m-%d')} - {end_date_obj.strftime('%Y-%m-%d')}\"\n",
    "\n",
    "    date_obj = dateparser.parse(text)\n",
    "    if date_obj:\n",
    "        return date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-23\n"
     ]
    }
   ],
   "source": [
    "text = \"–ù–∞–π–¥–∏ –¥–∞–Ω–Ω—ã–µ –∑–∞ 23 –æ–∫—Ç—è–±—Ä—è\"\n",
    "print(extract_date(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–∫—Å—Ç: –ü–æ–∫–∞–∂–∏ –∫—Ä—É–≥–æ–≤–æ–π –≥—Ä–∞—Ñ–∏–∫ –∑–∞ 8 –æ–∫—Ç—è–±—Ä—è 24 –≥–æ–¥–∞\n",
      "–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞/–∏–Ω—Ç–µ—Ä–≤–∞–ª: 2024-10-08\n",
      "\n",
      "–¢–µ–∫—Å—Ç: –û—Ç–æ–±—Ä–∞–∑–∏ –¥–∞–Ω–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥–æ–≤ –∑–∞ 08.10.24\n",
      "–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞/–∏–Ω—Ç–µ—Ä–≤–∞–ª: 2024-01-08\n",
      "\n",
      "–¢–µ–∫—Å—Ç: –ü—Ä–æ–¥–∞–∂–∏ —Å –∏—é–ª—è 23 –ø–æ —Å–µ–Ω—Ç—è–±—Ä—å 23\n",
      "–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞/–∏–Ω—Ç–µ—Ä–≤–∞–ª: 2023-07-01 - 2023-09-30\n",
      "\n",
      "–¢–µ–∫—Å—Ç: –°–¥–µ–ª–∞–π –∞–Ω–∞–ª–∏–∑ –∑–∞ –º–∞—Ä—Ç 2023\n",
      "–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞/–∏–Ω—Ç–µ—Ä–≤–∞–ª: 2023-03-01 - 2023-03-31\n",
      "\n",
      "–¢–µ–∫—Å—Ç: –ì—Ä–∞—Ñ–∏–∫ —Å –º–∞—è 22 –ø–æ –∞–≤–≥—É—Å—Ç 23\n",
      "–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞/–∏–Ω—Ç–µ—Ä–≤–∞–ª: 2022-05-01 - 2023-08-31\n",
      "\n",
      "–¢–µ–∫—Å—Ç: –ì—Ä–∞—Ñ–∏–∫ –∑–∞ –æ–∫—Ç—è–±—Ä–∏ 22 –≥–æ–¥–∞\n",
      "–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞/–∏–Ω—Ç–µ—Ä–≤–∞–ª: 2022-10-01 - 2022-10-31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"–ü–æ–∫–∞–∂–∏ –∫—Ä—É–≥–æ–≤–æ–π –≥—Ä–∞—Ñ–∏–∫ –∑–∞ 8 –æ–∫—Ç—è–±—Ä—è 24 –≥–æ–¥–∞\",\n",
    "    \"–û—Ç–æ–±—Ä–∞–∑–∏ –¥–∞–Ω–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥–æ–≤ –∑–∞ 08.10.24\",\n",
    "    \"–ü—Ä–æ–¥–∞–∂–∏ —Å –∏—é–ª—è 23 –ø–æ —Å–µ–Ω—Ç—è–±—Ä—å 23\",\n",
    "    \"–°–¥–µ–ª–∞–π –∞–Ω–∞–ª–∏–∑ –∑–∞ –º–∞—Ä—Ç 2023\",\n",
    "    \"–ì—Ä–∞—Ñ–∏–∫ —Å –º–∞—è 22 –ø–æ –∞–≤–≥—É—Å—Ç 23\",\n",
    "    \"–ì—Ä–∞—Ñ–∏–∫ –∑–∞ –æ–∫—Ç—è–±—Ä–∏ 22 –≥–æ–¥–∞\",\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    print(f\"–¢–µ–∫—Å—Ç: {text}\")\n",
    "    print(f\"–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞/–∏–Ω—Ç–µ—Ä–≤–∞–ª: {extract_date(text)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–∫—Å—Ç: 8 –æ–∫—Ç—è–±—Ä–∏ 2024\n",
      "–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞: 2024-10-08\n",
      "\n",
      "–¢–µ–∫—Å—Ç: —Å –∏—é–ª–∏ 23 –ø–æ —Å–µ–Ω—Ç—è–±—Ätt 23\n",
      "–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞: 2023-07-01 - 2023-07-31\n",
      "\n",
      "–¢–µ–∫—Å—Ç: 31 –¥–µ–∫–∞–±—Ä–∞–∞–ø 22\n",
      "–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞: 2022-12-31\n",
      "\n",
      "–¢–µ–∫—Å—Ç: —Ñ–µ–≤—Ñ—ã–ª—å 2024\n",
      "–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞: 2024-02-01 - 2024-02-29\n",
      "\n",
      "–¢–µ–∫—Å—Ç: —Ñ–µ–≤—Ñ—ã–ª—å 2025\n",
      "–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞: 2025-02-01 - 2025-02-28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"8 –æ–∫—Ç—è–±—Ä–∏ 2024\",\n",
    "    \"—Å –∏—é–ª–∏ 23 –ø–æ —Å–µ–Ω—Ç—è–±—Ätt 23\",\n",
    "    \"31 –¥–µ–∫–∞–±—Ä–∞–∞–ø 22\",\n",
    "    \"—Ñ–µ–≤—Ñ—ã–ª—å 2024\",\n",
    "    \"—Ñ–µ–≤—Ñ—ã–ª—å 2025\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(f\"–¢–µ–∫—Å—Ç: {text}\")\n",
    "    print(f\"–ù–∞–π–¥–µ–Ω–Ω–∞—è –¥–∞—Ç–∞: {extract_date(text)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sberbank-ai/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 398/398 [00:00<00:00, 7505.79 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 6250.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "data = pd.read_csv(\"prepared_dataset.csv\")\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "model_name = \"sberbank-ai/ruBert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    tokenized[\"label\"] = examples[\"label\"]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\–î–ù–°\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/75 [02:14<?, ?it/s]\n",
      "  9%|‚ñâ         | 7/75 [01:13<11:52, 10.48s/it]\n",
      "                                               \n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 25/75 [03:22<06:10,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.299873411655426, 'eval_runtime': 12.05, 'eval_samples_per_second': 8.299, 'eval_steps_per_second': 1.079, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 50/75 [06:47<03:06,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20822711288928986, 'eval_runtime': 12.639, 'eval_samples_per_second': 7.912, 'eval_steps_per_second': 1.029, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [10:29<00:00,  8.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2280440479516983, 'eval_runtime': 11.493, 'eval_samples_per_second': 8.701, 'eval_steps_per_second': 1.131, 'epoch': 3.0}\n",
      "{'train_runtime': 629.8436, 'train_samples_per_second': 1.896, 'train_steps_per_second': 0.119, 'train_loss': 0.27893513997395836, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model\\\\tokenizer_config.json',\n",
       " './model\\\\special_tokens_map.json',\n",
       " './model\\\\vocab.txt',\n",
       " './model\\\\added_tokens.json',\n",
       " './model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./model\")\n",
    "tokenizer.save_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\–î–ù–°\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "data = pd.read_csv(\"prepared_dataset.csv\")\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"./model\", tokenizer=\"./model\")\n",
    "semantic_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "data['embedding'] = data['text'].apply(lambda x: semantic_model.encode(x, convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "data = pd.read_csv(\"prepared_dataset.csv\")\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"./model\", tokenizer=\"./model\")\n",
    "\n",
    "def find_filters_with_classifier(text):\n",
    "    filters = []\n",
    "    label_map = {0: \"client\", 1: \"service\", 2: \"management\"}\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        prediction = classifier(row[\"text\"])[0]\n",
    "        predicted_label = int(prediction[\"label\"].replace(\"LABEL_\", \"\"))\n",
    "\n",
    "        if row[\"text\"].lower() in text.lower():\n",
    "            formatted_text = row[\"text\"].replace(\" \", \"\")\n",
    "            filters.append(f\"?{label_map[predicted_label]}={formatted_text}\")\n",
    "    \n",
    "    return filters\n",
    "\n",
    "\n",
    "def extract_entities_with_classifier(text):\n",
    "    \n",
    "    filters = find_filters_with_classifier(text)\n",
    "    \n",
    "    dashboards_data = get_dashboards_and_charts()\n",
    "    \n",
    "    dashboard_id, chart_id = find_dashboard_and_chart(text, dashboards_data)\n",
    "    \n",
    "    extracted_date = extract_date(text)\n",
    "\n",
    "    entities = {\n",
    "        \"dashboard\": dashboard_id,\n",
    "        \"chart\": chart_id,\n",
    "        \"filters\": filters,\n",
    "        \"date\": extracted_date\n",
    "    }\n",
    "    return entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_filters_with_classifier(text):\n",
    "    filters = []\n",
    "    label_map = {0: \"client\", 1: \"service\", 2: \"management\"}\n",
    "\n",
    "    query_embedding = semantic_model.encode(text, convert_to_tensor=True)\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        similarity_score = util.cos_sim(query_embedding, row['embedding']).item()\n",
    "\n",
    "        if similarity_score > 0.7:\n",
    "            prediction = classifier(row[\"text\"])[0]\n",
    "            predicted_label = int(prediction[\"label\"].replace(\"LABEL_\", \"\"))\n",
    "            formatted_text = row[\"text\"].replace(\" \", \"\")\n",
    "            filters.append(f\"?{label_map[predicted_label]}={formatted_text}\")\n",
    "\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_with_classifier(text):\n",
    "    filters = find_filters_with_classifier(text)\n",
    "    \n",
    "    extracted_date = extract_date(text)\n",
    "    \n",
    "    if extracted_date:\n",
    "        filters.append(f\"?date={extracted_date}\")\n",
    "    \n",
    "    dashboards_data = get_dashboards_and_charts()\n",
    "    \n",
    "    dashboard_id, chart_id = find_dashboard_and_chart(text, dashboards_data)\n",
    "    \n",
    "    entities = {\n",
    "        \"dashboard\": dashboard_id,\n",
    "        \"chart\": chart_id,\n",
    "        \"filters\": filters\n",
    "    }\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dashboard': 13, 'chart': 382, 'filters': ['?service=1–°:–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–•–æ–ª–¥–∏–Ω–≥–æ–º.–≠–Ω+', '?service=–°–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã–°–≠–î', '?date=2024-10-23']}\n"
     ]
    }
   ],
   "source": [
    "query = \"–ù–∞–π–¥–∏ –≥—Ä–∞—Ñ–∏–∫ '–ß–∞—Å—Ç–æ—Ç–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤' –Ω–∞ –¥–∞—à–±–æ—Ä–¥–µ 'Project EN+' –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ –ê–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ –£—Ä–∞–ª—å—Å–∫–∞—è —Ñ–æ–ª—å–≥–∞ —Å 1–°: –î–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç –∑–∞ 23 –æ–∫—Ç—è–±—Ä—è 24 –≥–æ–¥–∞\"\n",
    "# –ü–æ–∫–∞–∂–∏ –∫–æ–ª-–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ *—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ* –∑–∞ –ø–µ—Ä–∏–æ–¥\n",
    "# –ü–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–Ω—è—Ç—å –∫–∞–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –≥—Ä–∞—Ñ–∏–∫\n",
    "entities = extract_entities_with_classifier(query)\n",
    "\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:8088'\n",
    "\n",
    "username = 'admin'\n",
    "password = 'admin'\n",
    "\n",
    "auth_response = requests.post(f'{url}/api/v1/security/login', json={\n",
    "    'username': username,\n",
    "    'password': password,\n",
    "    'provider': 'db'\n",
    "})\n",
    "\n",
    "if auth_response.status_code == 200:\n",
    "    auth_token = auth_response.json()['access_token']\n",
    "    print(\"–ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ.\")\n",
    "else:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏: {auth_response.status_code} - {auth_response.text}\")\n",
    "    exit()\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {auth_token}'\n",
    "}\n",
    "\n",
    "def get_dashboards_and_charts():\n",
    "    dashboards_response = requests.get(f'{url}/api/v1/dashboard/', headers=headers)\n",
    "    if dashboards_response.status_code == 200:\n",
    "        dashboards = dashboards_response.json()['result']\n",
    "        dashboards_data = {}\n",
    "        for dashboard in dashboards:\n",
    "            dashboard_id = dashboard['id']\n",
    "            dashboard_title = dashboard['dashboard_title']\n",
    "            dashboards_data[dashboard_title] = {'id': dashboard_id, 'charts': []}\n",
    "            \n",
    "            charts_response = requests.get(f\"{url}/api/v1/dashboard/{dashboard_id}/charts\", headers=headers)\n",
    "            if charts_response.status_code == 200:\n",
    "                charts = charts_response.json()['result']\n",
    "                for chart in charts:\n",
    "                    dashboards_data[dashboard_title]['charts'].append({\n",
    "                        'id': chart['id'],\n",
    "                        'slice_name': chart['slice_name']\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {charts_response.status_code} - {charts_response.text}\")\n",
    "        return dashboards_data\n",
    "    else:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞—à–±–æ—Ä–¥–æ–≤: {dashboards_response.status_code} - {dashboards_response.text}\")\n",
    "        return None\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∞—à–±–æ—Ä–¥–∞ –∏ –≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É\n",
    "def find_dashboard_and_chart(query, dashboards_data):\n",
    "    query_lower = query.lower()\n",
    "    matched_dashboard = None\n",
    "    matched_dashboard_id = None\n",
    "    matched_chart = None\n",
    "    matched_chart_id = None\n",
    "\n",
    "    for dashboard_title, dashboard_info in dashboards_data.items():\n",
    "        if dashboard_title.lower() in query_lower:\n",
    "            matched_dashboard = dashboard_title\n",
    "            matched_dashboard_id = dashboard_info['id']\n",
    "            for chart in dashboard_info['charts']:\n",
    "                if chart['slice_name'].lower() in query_lower:\n",
    "                    matched_chart = chart['slice_name']\n",
    "                    matched_chart_id = chart['id']\n",
    "                    break\n",
    "            break\n",
    "\n",
    "    return matched_dashboard_id, matched_chart_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ.\n",
      "–°–ø–∏—Å–æ–∫ –¥–∞—à–±–æ—Ä–¥–æ–≤:\n",
      "- Project EN+ (ID: 13)\n",
      "  –ì—Ä–∞—Ñ–∏–∫–∏:\n",
      "    - –ß–∞—Å—Ç–æ—Ç–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤ (ID: 382)\n",
      "- COVID Vaccine Dashboard (ID: 11)\n",
      "  –ì—Ä–∞—Ñ–∏–∫–∏:\n",
      "    - Vaccine Candidates per Phase (ID: 114)\n",
      "    - Vaccine Candidates per Phase (ID: 115)\n",
      "    - Vaccine Candidates per Country & Stage (ID: 116)\n",
      "    - Vaccine Candidates per Country & Stage (ID: 117)\n",
      "    - Vaccine Candidates per Country (ID: 118)\n",
      "    - Vaccine Candidates per Country (ID: 119)\n",
      "    - Vaccine Candidates per Approach & Stage (ID: 120)\n",
      "- FCC New Coder Survey 2018 (ID: 10)\n",
      "  –ì—Ä–∞—Ñ–∏–∫–∏:\n",
      "    - Work Location Preference (ID: 93)\n",
      "    - Top 15 Languages Spoken at Home (ID: 94)\n",
      "    - ‚úàÔ∏è Relocation ability (ID: 95)\n",
      "    - Preferred Employment Style (ID: 96)\n",
      "    - Location of Current Developers (ID: 98)\n",
      "    - How much do you expect to earn? ($0 - 100k) (ID: 100)\n",
      "    - How do you prefer to work? (ID: 101)\n",
      "    - Highest degree held (ID: 102)\n",
      "    - First Time Developer & Commute Time (ID: 104)\n",
      "    - First Time Developer? (ID: 105)\n",
      "    - Ethnic Minority & Gender (ID: 106)\n",
      "    - Degrees vs Income (ID: 107)\n",
      "    - Current Developers: Is this your first development job? (ID: 108)\n",
      "    - Commute Time (ID: 110)\n",
      "    - Breakdown of Developer Type (ID: 111)\n",
      "    - Are you an ethnic minority in your city? (ID: 112)\n",
      "    - Age distribution of respondents (ID: 113)\n",
      "    - Number of Aspiring Developers (ID: 97)\n",
      "    - Last Year Income Distribution (ID: 99)\n",
      "    - Gender (ID: 103)\n",
      "    - Country of Citizenship (ID: 109)\n",
      "- Featured Charts (ID: 9)\n",
      "  –ì—Ä–∞—Ñ–∏–∫–∏:\n",
      "    - Word Cloud (ID: 69)\n",
      "    - Waterfall (ID: 70)\n",
      "    - TreeMap (ID: 71)\n",
      "    - Tree (ID: 72)\n",
      "    - Table (ID: 73)\n",
      "    - Sunburst (ID: 74)\n",
      "    - Scatter Plot (ID: 75)\n",
      "    - Sankey (ID: 76)\n",
      "    - Radar (ID: 77)\n",
      "    - Pivot Table (ID: 78)\n",
      "    - Pie (ID: 79)\n",
      "    - Mixed (ID: 80)\n",
      "    - Line (ID: 81)\n",
      "    - Heatmap (ID: 83)\n",
      "    - Graph (ID: 84)\n",
      "    - Gauge (ID: 85)\n",
      "    - Funnel (ID: 86)\n",
      "    - Bubble (ID: 87)\n",
      "    - Box Plot (ID: 88)\n",
      "    - Big Number with Trendline (ID: 89)\n",
      "    - Bar (ID: 91)\n",
      "    - Area (ID: 92)\n",
      "    - Histogram (ID: 82)\n",
      "    - Big Number (ID: 90)\n",
      "- Sales Dashboard (ID: 8)\n",
      "  –ì—Ä–∞—Ñ–∏–∫–∏:\n",
      "    - Total Revenue (ID: 49)\n",
      "    - Total Items Sold (By Product Line) (ID: 50)\n",
      "    - Total Items Sold (ID: 51)\n",
      "    - Seasonality of Revenue (per Product Line) (ID: 52)\n",
      "    - Revenue by Deal Size (ID: 53)\n",
      "    - Quarterly Sales (By Product Line) (ID: 54)\n",
      "    - Quarterly Sales (ID: 55)\n",
      "    - Proportion of Revenue by Product Line (ID: 56)\n",
      "    - Overall Sales (By Product Line) (ID: 57)\n",
      "    - Number of Deals (for each Combination) (ID: 58)\n",
      "- Slack Dashboard (ID: 7)\n",
      "  –ì—Ä–∞—Ñ–∏–∫–∏:\n",
      "    - Number of Members (ID: 63)\n",
      "    - Weekly Threads (ID: 60)\n",
      "    - Weekly Messages (ID: 61)\n",
      "    - Top Timezones (ID: 62)\n",
      "    - New Members per Month (ID: 64)\n",
      "    - Messages per Channel (ID: 65)\n",
      "    - Cross Channel Relationship heatmap (ID: 67)\n",
      "    - Members per Channel (ID: 66)\n",
      "    - Cross Channel Relationship (ID: 68)\n",
      "- Unicode Test (ID: 6)\n",
      "  –ì—Ä–∞—Ñ–∏–∫–∏:\n",
      "    - Unicode Cloud (ID: 59)\n",
      "- Video Game Sales (ID: 5)\n",
      "  –ì—Ä–∞—Ñ–∏–∫–∏:\n",
      "    - Total Sales per Market (Grouped by Genre) (ID: 39)\n",
      "    - Publishers With Most Titles (ID: 42)\n",
      "    - Popular Genres Across Platforms (ID: 43)\n",
      "    - # of Games That Hit 100k in Sales By Release Year (ID: 44)\n",
      "    - Most Dominant Platforms (ID: 45)\n",
      "    - Games per Genre (ID: 47)\n",
      "    - Games (ID: 48)\n",
      "    - Top 10 Games: Proportion of Sales in Markets (ID: 40)\n",
      "- deck.gl Demo (ID: 4)\n",
      "  –ì—Ä–∞—Ñ–∏–∫–∏:\n",
      "    - Deck.gl Scatterplot (ID: 342)\n",
      "    - Deck.gl Screen grid (ID: 343)\n",
      "    - Deck.gl Hexagons (ID: 344)\n",
      "    - Deck.gl Grid (ID: 345)\n",
      "    - Deck.gl Polygons (ID: 346)\n",
      "    - Deck.gl Arcs (ID: 347)\n",
      "    - Deck.gl Path (ID: 348)\n",
      "- Misc Charts (ID: 3)\n",
      "  –ì—Ä–∞—Ñ–∏–∫–∏:\n",
      "    - Parallel Coordinates (ID: 320)\n",
      "    - Mapbox Long/Lat (ID: 340)\n",
      "    - Birth in France by department in 2016 (ID: 341)\n",
      "- USA Births Names (ID: 2)\n",
      "  –ì—Ä–∞—Ñ–∏–∫–∏:\n",
      "    - Participants (ID: 321)\n",
      "    - Genders (ID: 322)\n",
      "    - Trends (ID: 323)\n",
      "    - Genders by State (ID: 324)\n",
      "    - Girls (ID: 325)\n",
      "    - Girl Name Cloud (ID: 326)\n",
      "    - Boys (ID: 327)\n",
      "    - Boy Name Cloud (ID: 328)\n",
      "    - Top 10 Girl Name Share (ID: 329)\n",
      "    - Top 10 Boy Name Share (ID: 330)\n",
      "    - Pivot Table v2 (ID: 331)\n",
      "- World Bank's Data (ID: 1)\n",
      "  –ì—Ä–∞—Ñ–∏–∫–∏:\n",
      "    - World's Population (ID: 311)\n",
      "    - Most Populated Countries (ID: 312)\n",
      "    - Growth Rate (ID: 313)\n",
      "    - % Rural (ID: 314)\n",
      "    - Life Expectancy VS Rural % (ID: 315)\n",
      "    - Rural Breakdown (ID: 316)\n",
      "    - World's Pop Growth (ID: 317)\n",
      "    - Box plot (ID: 318)\n",
      "    - Treemap (ID: 319)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# –£–∫–∞–∂–∏—Ç–µ URL –≤–∞—à–µ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ Superset\n",
    "url = 'http://localhost:8088'\n",
    "\n",
    "# –£–∫–∞–∂–∏—Ç–µ —Å–≤–æ–∏ —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "username = 'xx'\n",
    "password = 'xx'\n",
    "\n",
    "# –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –¥–æ—Å—Ç—É–ø–∞\n",
    "auth_response = requests.post(f'{url}/api/v1/security/login', json={\n",
    "    'username': username,\n",
    "    'password': password,\n",
    "    'provider': 'db'\n",
    "})\n",
    "\n",
    "if auth_response.status_code == 200:\n",
    "    auth_token = auth_response.json()['access_token']\n",
    "    print(\"–ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ.\")\n",
    "else:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏: {auth_response.status_code} - {auth_response.text}\")\n",
    "    exit()\n",
    "\n",
    "# –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {auth_token}'\n",
    "}\n",
    "\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–∞—à–±–æ—Ä–¥–æ–≤\n",
    "dashboards_response = requests.get(f'{url}/api/v1/dashboard/', headers=headers)\n",
    "\n",
    "if dashboards_response.status_code == 200:\n",
    "    dashboards = dashboards_response.json()\n",
    "    print(\"–°–ø–∏—Å–æ–∫ –¥–∞—à–±–æ—Ä–¥–æ–≤:\")\n",
    "    for dashboard in dashboards['result']:\n",
    "        print(f\"- {dashboard['dashboard_title']} (ID: {dashboard['id']})\")\n",
    "        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∞—à–±–æ—Ä–¥–∞\n",
    "        charts_response = requests.get(f\"{url}/api/v1/dashboard/{dashboard['id']}/charts\", headers=headers)\n",
    "        if charts_response.status_code == 200:\n",
    "            charts = charts_response.json()['result']\n",
    "            print(\"  –ì—Ä–∞—Ñ–∏–∫–∏:\")\n",
    "            for chart in charts:\n",
    "                print(f\"    - {chart['slice_name']} (ID: {chart['id']})\")\n",
    "        else:\n",
    "            print(f\"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {charts_response.status_code} - {charts_response.text}\")\n",
    "else:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞—à–±–æ—Ä–¥–æ–≤: {dashboards_response.status_code} - {dashboards_response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities_extractor.py\n",
    "\n",
    "import re\n",
    "import calendar\n",
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "import requests\n",
    "import spacy\n",
    "import dateparser\n",
    "from fuzzywuzzy import process\n",
    "from transformers import pipeline\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Spacy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞\n",
    "classifier = pipeline(\"text-classification\", model=\"./model\", tokenizer=\"./model\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "data = pd.read_csv(\"prepared_dataset.csv\")\n",
    "\n",
    "# –ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏ —Ç–æ–∫–µ–Ω–æ–º API\n",
    "class SupersetClient:\n",
    "    def __init__(self, superset_url, username, password):\n",
    "        self.superset_url = superset_url\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.token = None\n",
    "        self.token_expiry = None  # –í—Ä–µ–º—è –∏—Å—Ç–µ—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞\n",
    "        self.headers = None\n",
    "\n",
    "    def authenticate(self):\n",
    "        auth_response = requests.post(f'{self.superset_url}/api/v1/security/login', json={\n",
    "            'username': self.username,\n",
    "            'password': self.password,\n",
    "            'provider': 'db'\n",
    "        })\n",
    "\n",
    "        if auth_response.status_code == 200:\n",
    "            auth_data = auth_response.json()\n",
    "            self.token = auth_data['access_token']\n",
    "            # –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —Ç–æ–∫–µ–Ω –¥–µ–π—Å—Ç–≤—É–µ—Ç 1 —á–∞—Å\n",
    "            self.token_expiry = datetime.now() + timedelta(hours=1)\n",
    "            self.headers = {\n",
    "                'Authorization': f'Bearer {self.token}'\n",
    "            }\n",
    "            print(\"–ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞.\")\n",
    "        else:\n",
    "            raise Exception(f\"–û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏: {auth_response.status_code} - {auth_response.text}\")\n",
    "\n",
    "    def ensure_authenticated(self):\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –ª–∏ —Ç–æ–∫–µ–Ω\n",
    "        if not self.token or datetime.now() >= self.token_expiry:\n",
    "            print(\"–¢–æ–∫–µ–Ω –∏—Å—Ç–µ–∫ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è.\")\n",
    "            self.authenticate()\n",
    "\n",
    "    def get(self, endpoint):\n",
    "        self.ensure_authenticated()\n",
    "        response = requests.get(f'{self.superset_url}{endpoint}', headers=self.headers)\n",
    "        # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω –∏—Å—Ç–µ–∫ –∏–ª–∏ –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω, –ø–æ–≤—Ç–æ—Ä—è–µ–º –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é\n",
    "        if response.status_code == 401:\n",
    "            print(\"–¢–æ–∫–µ–Ω –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è.\")\n",
    "            self.authenticate()\n",
    "            response = requests.get(f'{self.superset_url}{endpoint}', headers=self.headers)\n",
    "        return response\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–æ–∫ –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö –º–µ—Å—è—Ü–µ–≤\n",
    "def correct_month(input_month):\n",
    "    month_mapping = {\n",
    "        \"—è–Ω–≤–∞—Ä—å\": \"01\", \"—è–Ω–≤–∞—Ä—è\": \"01\",\n",
    "        \"—Ñ–µ–≤—Ä–∞–ª—å\": \"02\", \"—Ñ–µ–≤—Ä–∞–ª—è\": \"02\",\n",
    "        \"–º–∞—Ä—Ç\": \"03\", \"–º–∞—Ä—Ç–∞\": \"03\",\n",
    "        \"–∞–ø—Ä–µ–ª—å\": \"04\", \"–∞–ø—Ä–µ–ª—è\": \"04\",\n",
    "        \"–º–∞–π\": \"05\", \"–º–∞—è\": \"05\",\n",
    "        \"–∏—é–Ω—å\": \"06\", \"–∏—é–Ω—è\": \"06\",\n",
    "        \"–∏—é–ª—å\": \"07\", \"–∏—é–ª—è\": \"07\",\n",
    "        \"–∞–≤–≥—É—Å—Ç\": \"08\", \"–∞–≤–≥—É—Å—Ç–∞\": \"08\",\n",
    "        \"—Å–µ–Ω—Ç—è–±—Ä—å\": \"09\", \"—Å–µ–Ω—Ç—è–±—Ä—è\": \"09\",\n",
    "        \"–æ–∫—Ç—è–±—Ä—å\": \"10\", \"–æ–∫—Ç—è–±—Ä—è\": \"10\",\n",
    "        \"–Ω–æ—è–±—Ä—å\": \"11\", \"–Ω–æ—è–±—Ä—è\": \"11\",\n",
    "        \"–¥–µ–∫–∞–±—Ä—å\": \"12\", \"–¥–µ–∫–∞–±—Ä—è\": \"12\"\n",
    "    }\n",
    "\n",
    "    corrected_month = process.extractOne(input_month.lower(), month_mapping.keys())\n",
    "    return month_mapping.get(corrected_month[0]) if corrected_month else None\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞\n",
    "def extract_date(text):\n",
    "    doc = nlp(text)\n",
    "    dates = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
    "    if dates:\n",
    "        for date_str in dates:\n",
    "            date_obj = dateparser.parse(date_str)\n",
    "            if date_obj:\n",
    "                return date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    regex_patterns = {\n",
    "        \"interval\": r'\\b—Å\\s+([–∞-—è–ê-–Ø]+)\\s+(\\d{2})\\s+–ø–æ\\s+([–∞-—è–ê-–Ø]+)\\s+(\\d{2})\\b',\n",
    "        \"single_date\": [\n",
    "            r'\\b(\\d{1,2})\\.(\\d{1,2})\\.(\\d{2,4})\\b',\n",
    "            r'\\b(\\d{1,2})\\s+([–∞-—è–ê-–Ø]+)\\s+(\\d{2,4})\\b',\n",
    "            r'\\b(\\d{1,2})\\s+([–∞-—è–ê-–Ø]+)\\s+(\\d{2})\\s+–≥–æ–¥–∞\\b'\n",
    "        ],\n",
    "        \"month_year\": r'\\b(?<!\\d\\s)([–∞-—è–ê-–Ø]+)\\s+(\\d{2,4})\\b'\n",
    "    }\n",
    "\n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –¥–∞—Ç\n",
    "    interval_match = re.search(regex_patterns[\"interval\"], text, re.IGNORECASE)\n",
    "    if interval_match:\n",
    "        start_month_name, start_year, end_month_name, end_year = interval_match.groups()\n",
    "        start_month = correct_month(start_month_name)\n",
    "        end_month = correct_month(end_month_name)\n",
    "        if start_month and end_month:\n",
    "            if len(start_year) == 2:\n",
    "                start_year = f\"20{start_year}\"\n",
    "            if len(end_year) == 2:\n",
    "                end_year = f\"20{end_year}\"\n",
    "            start_date_obj = date(int(start_year), int(start_month), 1)\n",
    "            last_day = calendar.monthrange(int(end_year), int(end_month))[1]\n",
    "            end_date_obj = date(int(end_year), int(end_month), last_day)\n",
    "            return f\"{start_date_obj.strftime('%Y-%m-%d')} - {end_date_obj.strftime('%Y-%m-%d')}\"\n",
    "\n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –¥–∞—Ç\n",
    "    for pattern in regex_patterns[\"single_date\"]:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            if len(match.groups()) == 3:\n",
    "                day, month_or_name, year = match.groups()\n",
    "                month = correct_month(month_or_name)\n",
    "                if month:\n",
    "                    if len(year) == 2:\n",
    "                        year = f\"20{year}\"\n",
    "                    return f\"{year}-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "\n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–µ—Å—è—Ü–∞ –∏ –≥–æ–¥–∞\n",
    "    month_year_match = re.search(regex_patterns[\"month_year\"], text, re.IGNORECASE)\n",
    "    if month_year_match:\n",
    "        month_name, year = month_year_match.groups()\n",
    "        month = correct_month(month_name)\n",
    "        if month:\n",
    "            if len(year) == 2:\n",
    "                year = f\"20{year}\"\n",
    "            start_date_obj = date(int(year), int(month), 1)\n",
    "            last_day = calendar.monthrange(int(year), int(month))[1]\n",
    "            end_date_obj = date(int(year), int(month), last_day)\n",
    "            return f\"{start_date_obj.strftime('%Y-%m-%d')} - {end_date_obj.strftime('%Y-%m-%d')}\"\n",
    "\n",
    "    date_obj = dateparser.parse(text)\n",
    "    if date_obj:\n",
    "        return date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return None\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –¥–∞—à–±–æ—Ä–¥–æ–≤ –∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏–∑ Superset\n",
    "def get_dashboards_and_charts(superset_client):\n",
    "    dashboards_response = superset_client.get('/api/v1/dashboard/')\n",
    "    if dashboards_response.status_code == 200:\n",
    "        dashboards = dashboards_response.json()['result']\n",
    "        dashboards_data = {}\n",
    "        for dashboard in dashboards:\n",
    "            dashboard_id = dashboard['id']\n",
    "            dashboard_title = dashboard['dashboard_title']\n",
    "            dashboards_data[dashboard_title] = {'id': dashboard_id, 'charts': []}\n",
    "\n",
    "            charts_response = superset_client.get(f\"/api/v1/dashboard/{dashboard_id}/charts\")\n",
    "            if charts_response.status_code == 200:\n",
    "                charts = charts_response.json()['result']\n",
    "                for chart in charts:\n",
    "                    dashboards_data[dashboard_title]['charts'].append({\n",
    "                        'id': chart['id'],\n",
    "                        'slice_name': chart['slice_name']\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {charts_response.status_code} - {charts_response.text}\")\n",
    "        return dashboards_data\n",
    "    else:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞—à–±–æ—Ä–¥–æ–≤: {dashboards_response.status_code} - {dashboards_response.text}\")\n",
    "        return None\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∞—à–±–æ—Ä–¥–∞ –∏ –≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É\n",
    "def find_dashboard_and_chart(query, dashboards_data, similarity_threshold=0.5, fuzz_threshold=50):\n",
    "    query_lower = query.lower()\n",
    "    lemmatized_query = lemmatize_text(query)  # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞\n",
    "    query_embedding = semantic_model.encode(lemmatized_query, convert_to_tensor=True)\n",
    "\n",
    "    matched_dashboard_id = None\n",
    "    matched_chart_id = None\n",
    "\n",
    "    max_dashboard_score = 0\n",
    "    max_chart_score = 0\n",
    "\n",
    "    for dashboard_title, dashboard_info in dashboards_data.items():\n",
    "        lemmatized_dashboard_title = lemmatize_text(dashboard_title)  # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –¥–∞—à–±–æ—Ä–¥–∞\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å –¥–∞—à–±–æ—Ä–¥–æ–º\n",
    "        semantic_similarity = util.cos_sim(query_embedding, semantic_model.encode(lemmatized_dashboard_title, convert_to_tensor=True)).item()\n",
    "        fuzzy_score = fuzz.token_set_ratio(query_lower, dashboard_title.lower())\n",
    "\n",
    "        # –°—Ä–µ–¥–Ω—è—è –º–µ—Ç—Ä–∏–∫–∞ (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤–µ—Å–∞)\n",
    "        dashboard_score = (semantic_similarity * 100 + fuzzy_score) / 2\n",
    "\n",
    "        if dashboard_score > max_dashboard_score and dashboard_score >= fuzz_threshold:\n",
    "            max_dashboard_score = dashboard_score\n",
    "            matched_dashboard_id = dashboard_info['id']\n",
    "\n",
    "            # –ü–æ–∏—Å–∫ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—É—â–µ–≥–æ –¥–∞—à–±–æ—Ä–¥–∞\n",
    "            for chart in dashboard_info['charts']:\n",
    "                lemmatized_chart_name = lemmatize_text(chart['slice_name'])  # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞\n",
    "\n",
    "                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å –≥—Ä–∞—Ñ–∏–∫–æ–º\n",
    "                chart_semantic_similarity = util.cos_sim(query_embedding, semantic_model.encode(lemmatized_chart_name, convert_to_tensor=True)).item()\n",
    "                chart_fuzzy_score = fuzz.token_set_ratio(query_lower, chart['slice_name'].lower())\n",
    "\n",
    "                # –°—Ä–µ–¥–Ω—è—è –º–µ—Ç—Ä–∏–∫–∞\n",
    "                chart_score = (chart_semantic_similarity * 100 + chart_fuzzy_score) / 2\n",
    "\n",
    "                if chart_score > max_chart_score and chart_score >= fuzz_threshold:\n",
    "                    max_chart_score = chart_score\n",
    "                    matched_chart_id = chart['id']\n",
    "\n",
    "    return matched_dashboard_id, matched_chart_id\n",
    "\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "data['embedding'] = data['text'].apply(lambda x: semantic_model.encode(x, convert_to_tensor=True))\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "data['lemmatized_text'] = data['text'].apply(lemmatize_text)\n",
    "\n",
    "def find_filters_with_classifier(text, similarity_threshold=0.6, fuzz_threshold=60):\n",
    "    filters = []\n",
    "    label_map = {0: \"client\", 1: \"service\", 2: \"management\"}\n",
    "\n",
    "    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞\n",
    "    lemmatized_query = lemmatize_text(text)\n",
    "\n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞\n",
    "    text_embedding = semantic_model.encode(lemmatized_query, convert_to_tensor=True)\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç—Ä–æ–∫–∏\n",
    "        lemmatized_row_text = row['lemmatized_text']\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å\n",
    "        similarity = util.cos_sim(text_embedding, row['embedding']).item()\n",
    "\n",
    "        if similarity >= similarity_threshold:\n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—á–µ—Ç–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å\n",
    "            fuzzy_score = fuzz.token_set_ratio(lemmatized_row_text.lower(), lemmatized_query.lower())\n",
    "            if fuzzy_score >= fuzz_threshold:\n",
    "                # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–æ–∫–∏\n",
    "                prediction = classifier(row[\"text\"])[0]\n",
    "                predicted_label = int(prediction[\"label\"].replace(\"LABEL_\", \"\"))\n",
    "                filters.append(f\"?{label_map[predicted_label]}={row['text']}\")\n",
    "\n",
    "    return filters\n",
    "\n",
    "# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π\n",
    "def extract_entities(text, superset_url, superset_username, superset_password):\n",
    "    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∏–µ–Ω—Ç–∞ Superset\n",
    "    superset_client = SupersetClient(superset_url, superset_username, superset_password)\n",
    "\n",
    "    # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –¥–∞—à–±–æ—Ä–¥–∞—Ö –∏ –≥—Ä–∞—Ñ–∏–∫–∞—Ö\n",
    "    dashboards_data = get_dashboards_and_charts(superset_client)\n",
    "\n",
    "    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—à–±–æ—Ä–¥–∞ –∏ –≥—Ä–∞—Ñ–∏–∫–∞\n",
    "    dashboard_id, chart_id = find_dashboard_and_chart(text, dashboards_data)\n",
    "\n",
    "    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤\n",
    "    filters = find_filters_with_classifier(text)\n",
    "\n",
    "    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã\n",
    "    extracted_date = extract_date(text)\n",
    "\n",
    "    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞—Ç—ã –≤ —Ñ–∏–ª—å—Ç—Ä—ã\n",
    "    if extracted_date:\n",
    "        if ' - ' in extracted_date:\n",
    "            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –¥–∞—Ç\n",
    "            start_date, end_date = extracted_date.split(' - ')\n",
    "            filters.append(f\"?start_date={start_date}\")\n",
    "            filters.append(f\"?end_date={end_date}\")\n",
    "        else:\n",
    "            filters.append(f\"?date={extracted_date}\")\n",
    "\n",
    "    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "    entities = {\n",
    "        \"dashboard_id\": dashboard_id,\n",
    "        \"chart_id\": chart_id,\n",
    "        \"filters\": filters,\n",
    "    }\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Query: –ù–∞–π–¥–∏ –≥—Ä–∞—Ñ–∏–∫ '–ß–∞—Å—Ç–æ—Ç–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤' –Ω–∞ –¥–∞—à–±–æ—Ä–¥–µ Project En+ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ –£—Ä–∞–ª—å—Å–∫–∏–π —Ñ–æ–ª—å–≥–∞ —Å 1–°: 1–°:–î–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç –∑–∞ 23 –æ–∫—Ç—è–±—Ä—è 24 –≥–æ–¥\n",
      "['?client=–ê–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ –£—Ä–∞–ª—å—Å–∫–∞—è —Ñ–æ–ª—å–≥–∞', '?service=1–°: –î–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç']\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "def normalize_text(text):\n",
    "    replacements = [\n",
    "        r\"\\b(–∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ|–∞–æ|–æ–±—â–µ—Å—Ç–≤–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é|–æ–æ–æ)\\b\",\n",
    "        r\"\\b(–ø—É–±–ª–∏—á–Ω–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ|–ø–∞–æ)\\b\"\n",
    "    ]\n",
    "    for pattern in replacements:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "    return text.strip()\n",
    "\n",
    "def correct_spelling(text, vocabulary, threshold=80):\n",
    "    corrected_words = []\n",
    "    for word in text.split():\n",
    "        # –ò—â–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–µ–µ —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–∞—Ä–µ\n",
    "        match = process.extractOne(word, vocabulary, scorer=fuzz.token_set_ratio)\n",
    "        if match and match[1] >= threshold:\n",
    "            corrected_words.append(match[0])\n",
    "        else:\n",
    "            corrected_words.append(word) \n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ, –¥–æ–±–∞–≤–ª—è–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é\n",
    "data['normalized_text'] = data['text'].apply(normalize_text)\n",
    "data['lemmatized_text'] = data['normalized_text'].apply(lemmatize_text)\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–ø–µ—á–∞—Ç–æ–∫\n",
    "vocabulary = set(data['normalized_text'].str.split().sum())\n",
    "\n",
    "def find_filters_with_classifier(text, similarity_threshold=0.5, fuzz_threshold=90, spelling_threshold=80):\n",
    "    filters = []\n",
    "    label_map = {0: \"client\", 1: \"service\", 2: \"management\"}\n",
    "\n",
    "    normalized_query = normalize_text(text)\n",
    "    corrected_query = correct_spelling(normalized_query, vocabulary, spelling_threshold)\n",
    "    print(f\"Corrected Query: {corrected_query}\")\n",
    "\n",
    "    lemmatized_query = lemmatize_text(corrected_query)\n",
    "\n",
    "    text_embedding = semantic_model.encode(lemmatized_query, convert_to_tensor=True)\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        lemmatized_row_text = row['lemmatized_text']\n",
    "\n",
    "        similarity = util.cos_sim(text_embedding, row['embedding']).item()\n",
    "\n",
    "        if similarity >= similarity_threshold:\n",
    "            fuzzy_score = fuzz.token_set_ratio(lemmatized_row_text.lower(), lemmatized_query.lower())\n",
    "            if fuzzy_score >= fuzz_threshold:\n",
    "                prediction = classifier(row[\"text\"])[0]\n",
    "                predicted_label = int(prediction[\"label\"].replace(\"LABEL_\", \"\"))\n",
    "                filters.append(f\"?{label_map[predicted_label]}={row['text']}\")\n",
    "\n",
    "    return filters\n",
    "\n",
    "print(find_filters_with_classifier(\"–ù–∞–π–¥–∏ –≥—Ä–∞—Ñ–∏–∫ '–ß–∞—Å—Ç–æ—Ç–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤' –Ω–∞ –¥–∞—à–±–æ—Ä–¥–µ 'Project EN+' –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ —É—Ä–∞–ª—å—Å–∫–∞—è —Ñ–æ–ª—å–≥–∞ —Å 1–°: –î–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç –∑–∞ 23 –æ–∫—Ç—è–±—Ä—è 24 –≥–æ–¥–∞\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–æ–∫–µ–Ω –∏—Å—Ç–µ–∫ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è.\n",
      "–ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞.\n",
      "{'dashboard_id': 13, 'chart_id': 382, 'filters': ['?service=1–°: –î–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç', '?service=1–°:–î–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç –ö–í–°–£', '?date=2024-10-23']}\n"
     ]
    }
   ],
   "source": [
    "query = \"–ù–∞–π–¥–∏ –≥—Ä–∞—Ñ–∏–∫ '–ß–∞—Å—Ç–æ—Ç–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤' –Ω–∞ –¥–∞—à–±–æ—Ä–¥–µ 'Project EN+' –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ —É—Ä–∞–ª—å—Å–∫–∞—è —Ñ–æ–ª—å–≥–∞ —Å 1–°: –î–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç –∑–∞ 23 –æ–∫—Ç—è–±—Ä—è 24 –≥–æ–¥–∞\"\n",
    "superset_url = 'http://localhost:8088/'\n",
    "superset_username = 'admin'\n",
    "superset_password = 'admin'\n",
    "\n",
    "entities = extract_entities(query, superset_url, superset_username, superset_password)\n",
    "\n",
    "print(entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
